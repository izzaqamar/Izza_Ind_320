{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b26cd5e",
   "metadata": {},
   "source": [
    "# Project IND-320\n",
    "**Name** : _Izza Qamar_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ef4c72",
   "metadata": {},
   "source": [
    "## Links\n",
    " - **GitHub Repository** : https://github.com/izzaqamar/Izza_Ind_320.git  \n",
    "\n",
    " - **Streamlit App** : https://izza-ind320.streamlit.app/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a923c9",
   "metadata": {},
   "source": [
    "# Project Overview and AI Usage "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535c9498",
   "metadata": {},
   "source": [
    "## Deliverable 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbee0d68",
   "metadata": {},
   "source": [
    "### AI tools\n",
    "I have used **chatgpt** on multiple occassions. Initially it was used to understand how to connect my visual studio with github and then connecting github to streamlit. Then as the work progressed the major help I took was from chatgpt regarding the building of multipage streamlit app.\n",
    "I used it to understand how graphs are created and how we can add customizations to our page like the sliders and selectbox. Lastly, it was used to create a 'Stay Tuned' text on the last page. Lastly I used chatgpt to understand how to export jupyter notebook as a pdf.Copilot was not used during the tasks.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e6b25e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Project Overview\n",
    "This section contains brief desciption of jupyter notebook and streamlit app. \n",
    "  \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81315a2b",
   "metadata": {},
   "source": [
    "#### Jupter Notebook\n",
    "The project is comprised of two major tasks. Creating a jupyter notebook and a streamlit app.\n",
    "\n",
    " - For the jupyter notebook, I firstly created an environment for reproducibility. Then an outline of the major topics including the headings which include the links to my github repository and streamlit app. \n",
    " - Secondly, the section of 'Project overview and AI' section highlights the usage of different AI tools used in this project and a brief description of tasks performed in this project. \n",
    " - Thirdly, the section 'Jupyter Work' includes the detailed code files. In this section, for the first task I loaded the file in form of dataframe from the csv using pandas and then displayed the descriptive statistics to get an overview of  data and the type of data it contains for easier understanding. For the second task I created two different kinds of plots. To plot each column separately, I used matplotlib and a for loop to create a line plot of each column against time (fixed as index of the dataframe). To plot all columns collectively, the numeric columns of the dataframe were first normalized as they had different scales. Then after adding the time column, I used a for loop to create a single line plot that overlays all normalized weather variables over time, with time on the x-axis and normalized measurement values on the y-axis. Each variable is shown as a different colored line, and I added the legend so it helps to identify which line corresponds to which variable.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764051ed",
   "metadata": {},
   "source": [
    "#### Streamlit app\n",
    "I created a multipage streamlit app where the entrypoint file named 'Homepage.py' and a folder named 'pages' share the same folder. Then in the folder 'pages' I added all the required pages namely 'First Months Insights' , 'Data Visualization' and 'Stay Tuned' and given them the number for the order of appearance in the sidebar according to my preference.\n",
    "\n",
    "- 'First month insights' page displays the csv file in form of table using st.data_editor. Then the dataframe is filtered for January and restructured so that each column variable becomes a row and its respective values are displayed as a list in adjacent value column. LinechartColumn was used to create a table consisting of mini line charts of each weather variable.\n",
    "- 'Data visualization' page mainly consists of three parts. Firstly, the data is read using caching and a slider is displayed, to select range of months to visualize. Subsequently, months in between the range are also selected. After this the dataframe is converted from wide format to long format (easier to plot using altair) and months column is added in the dataframe to filter the dataframe by the selected months. Secondly, a selectbox is displayed so the user can select the variable or all variables to visualize at the same time. Using the input from user I wrote a conditional statement that will filter the dataframe that is already filtered monthly by variable name from user and then create a plot accordingly using altair. The plots are customized by adding relevant labels,header and axis titles. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9a82d0",
   "metadata": {},
   "source": [
    "## Deliverable 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f749ae8e",
   "metadata": {},
   "source": [
    "### AI tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ea09b5",
   "metadata": {},
   "source": [
    "### Project Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d168ccb3",
   "metadata": {},
   "source": [
    "# Jupyter Work\n",
    "- The following sections contain the jupyter tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f773a9ca",
   "metadata": {},
   "source": [
    "## Deliverable 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57640602",
   "metadata": {},
   "source": [
    "### Data Loading and Reading\n",
    " - In this section, we read and load the data from csv file and display descriptive statistics to see how the data looks.It provides the snapshot of the distribution, central tendency, spread, and completeness of your data. This helps us understand the overall behavior and quality of your dataset.\n",
    " - We also see the type of the columns in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587112c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#Reading csv as dataframe\n",
    "open_meteo_df=pd.read_csv(r\"D:\\NMBU\\semester_1\\IND-320\\Izza_Ind_320\\open-meteo-subset.csv\",parse_dates=['time'])\n",
    "#Displaying descriptive statistics of dataframe \n",
    "print(open_meteo_df.info())\n",
    "print(open_meteo_df.drop(columns=['time']).describe().round(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ad708a",
   "metadata": {},
   "source": [
    "###  Plots\n",
    " - In this section, we have following plots:\n",
    "    - We plot each column separately as a function of time.\n",
    "    - We collectively plot all columns together as a function of time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ac7ef5",
   "metadata": {},
   "source": [
    "#### Column Plot\n",
    "- Plotting each column separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e1c055",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#Setting time as the index of df\n",
    "open_meteo_df.set_index('time', inplace=True)\n",
    "#Creating df of numeric columns\n",
    "numeric_cols = open_meteo_df.select_dtypes(include='number')\n",
    "\n",
    "#Plotting each column separately\n",
    "for i, col in enumerate(numeric_cols.columns):\n",
    "    plt.figure(figsize=(15, 4))\n",
    "    plt.plot(open_meteo_df.index, numeric_cols[col])\n",
    "    plt.title(f\"{col} over Time\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(col)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f81d997",
   "metadata": {},
   "source": [
    "#### Collective Plot\n",
    "- Plotting all columns together in a plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d66cb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#Normalizing the numeric dataframe as they are of different scale.\n",
    "normalized_df = (numeric_cols - numeric_cols.min()) / (numeric_cols.max() - numeric_cols.min())\n",
    "\n",
    "#Adding the time column to normalized dataframe\n",
    "normalized_df['Time'] = open_meteo_df.index\n",
    "\n",
    "#Creating the figure\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "for col in normalized_df.columns.drop('Time'):\n",
    "    plt.plot(normalized_df['Time'], normalized_df[col], label=col)\n",
    "\n",
    "plt.title(\"Normalized Weather Data Over Time\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Normalized Value\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f2d164",
   "metadata": {},
   "source": [
    "## Deliverable 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602b49ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cassandra setup\n",
    "from cassandra.cluster import Cluster\n",
    "cluster = Cluster(['localhost'], port=9042)\n",
    "session = cluster.connect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775cc3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spark-Cassandra setup\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('SparkCassandraApp').\\\n",
    "    config('spark.jars.packages', 'com.datastax.spark:spark-cassandra-connector_2.12:3.5.1').\\\n",
    "    config('spark.cassandra.connection.host', 'localhost').\\\n",
    "    config('spark.sql.extensions', 'com.datastax.spark.connector.CassandraSparkExtensions').\\\n",
    "    config('spark.sql.catalog.mycatalog', 'com.datastax.spark.connector.datasource.CassandraCatalog').\\\n",
    "    config('spark.cassandra.connection.port', '9042').getOrCreate()\n",
    "print(\"Spark session created successfully:\", spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8c8f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mongo db\n",
    "from pymongo.mongo_client import MongoClient\n",
    "from pymongo.server_api import ServerApi\n",
    "\n",
    "username,password = open(r\"D:\\NMBU\\semester_1\\IND-320\\mongodb_password.txt\").read().strip().split(',')\n",
    "\n",
    "#uri = f\"mongodb+srv://{username}:{password}@cluster0.0crxtmx.mongodb.net/?retryWrites=true&w=majority&appName=app-cluster\"\n",
    "uri = f\"mongodb+srv://{username}:{password}@app-cluster.ihj1zbx.mongodb.net/?retryWrites=true&w=majority&appName=app-cluster\"\n",
    "\n",
    "# Create a new client and connect to the server\n",
    "client = MongoClient(uri, server_api=ServerApi('1'))\n",
    "\n",
    "# Send a ping to confirm a successful connection\n",
    "try:\n",
    "    client.admin.command('ping')\n",
    "    print(\"Pinged your deployment. You successfully connected to MongoDB!\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe480ae",
   "metadata": {},
   "source": [
    "### Data from Api"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f163006b",
   "metadata": {},
   "source": [
    "### Pie chart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11288469",
   "metadata": {},
   "source": [
    "### Line plot"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ind320_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
