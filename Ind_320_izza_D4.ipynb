{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3d5aab0",
   "metadata": {},
   "source": [
    "# Project IND-320\n",
    "**Name** : _Izza Qamar_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a390258d",
   "metadata": {},
   "source": [
    "## Links\n",
    " - **GitHub Repository** : https://github.com/izzaqamar/Izza_Ind_320.git  \n",
    "\n",
    " - **Streamlit App** : https://izza-ind320.streamlit.app/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08fbb9d",
   "metadata": {},
   "source": [
    "# Deliverable 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5742801e",
   "metadata": {},
   "source": [
    "## Project Overview and AI Usage "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae84cad",
   "metadata": {},
   "source": [
    "### AI tools\n",
    "- For Jupyter task, I have used chatgpt to understand how I can extend my previous api_call function to multiple years for production and  consumption.\n",
    "- For Streamlit, I have used both chatgpt and copilot. Initially to understand use of folium as I have never used it before and to add outlines and Choropleth to the map. Then to understand how to plot wind rose and snow drift. Then for the forecasting part I used it to read about Sarimax and its parameters and took help in building logic for the page. I also took its help to understand how I can incorporate the confidence interval on my graph for forecasting. Finally, I took its help to build my homepage and how I can set the navigation sidebar the way I wanted to organize pages in groups and subgroups. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408f47a0",
   "metadata": {},
   "source": [
    "### Project Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c26897",
   "metadata": {},
   "source": [
    "#### Jupter Notebook\n",
    "- I created new jupyter notebook for this deliverable. I began by fetching data from the API for production and consumption on similar patter. I first defined the required parameters. The API allows fetching data for only one month per call, so to retrieve data for multiple years, I added a for loop for list of years and then I created a month_range function to generate all months and passed them using a for loop. The data for October had to be split into two parts as I was getting error for 31st Oct. I used another for loop to access productionPerGroupMbaHour/consumptionPerGroupMbaHour for all areas. Then I used extend() to add all fetched data per request into the production_data to get list of dictionaries.\n",
    "- I converted this list of dictionaries into a pandas DataFrame and formatted the time columns to UTC.\n",
    "- I set up Cassandra, connected to it, and created the keyspace and table using case-sensitive formatting.\n",
    "- I set up a Spark-Cassandra connection, converted the pandas DataFrame to a Spark DataFrame, and inserted it into Cassandra(just appended the new production data and added all the new consumption data). I then extracted the required columns from Cassandra using Spark. I repeated these steps for production and consumption datasets.\n",
    "- Finally, I connected to MongoDB to my collection ind320_production_table_d4 and ind320_consumption_table, converted the Spark data to pandas DataFrame, then dictionaries, and inserted it into MongoDB.\n",
    "- **Bonus Point** : I calculated the monthly snow drift and plotted it separately and together with the yearly snow drift."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba87106",
   "metadata": {},
   "source": [
    "#### Streamlit app\n",
    "- Structural changes: The data is now either acquired from MongoDb or API. No csv files are used. \n",
    "\n",
    "1. Sections: I edited the homepage to have a custom sidebar navigation. It has sections 'energy' based on energy data fetched from MongoDB, 'weather' based on weather data fetch from open-meteo api and Cross domain which is common for both datasets.\n",
    "2. Subgroup: For energy data, there is subgroup of visualization containing insights in form of Pie plot, line plot and maps page. The analysis subgroup has page STL/Spectogram and forecasting subgroup has Sarimax dynamic forecasting. The years range for energy section is 2021-2024.\n",
    "3. Subgroup: For weather data, there are two subgroups which are Visualization(containing Data Insights) and Analysis(containing Outlier/Anomalies page). The years range for weather section is 2000-2024.\n",
    "4. Subgroup: For Cross Domain there is only one subgroup and a page containing sliding window correlation. The years range for Cross Domain section is 2021-2024.\n",
    "\n",
    "- Deliverable-4:\n",
    "For this deliverable I created a utils.py file which contains my connection function for MongoDB and Api so its much cleaner and easier to use. I have added 4 new pages.\n",
    "1. Maps and Energy Choropleth: Two columns. In the first column, I have added the map using the said geojson from folium. The 5 price areas borders are marked and highlights the price area border with a different color on a click. The click also save latitude and longitude in session state for other pages to use it. The second column gives user to select dataset and respective group and see the choropleth of quantitykwh in the user selected time for (2021-2024). The data is fetched only for selected time.\n",
    "2. Snow Drift: The code is copied from the file provided. It fetches data from api for the location selected on map page and redirect to maps page if no location selected. It fetches data for 1 July fo year to 30 June of next year and the user can select the year range from slider (2000-2024). I have asked user to pick whether they want to see montly or yearly snow drift bar plots. Then the wind rose plot is also added.\n",
    "3. Sliding Window Correlation: It fetches data from api for location selected and redirects to map page if no location selected. Then fetches data from api and mongodb for the user selected year (2021-2024). Some Ui elements are provided to make it interactive so user can explore the effect of weather properties on energy production(quantity kwh) or consumption (quantity kwh).\n",
    "4. Forecasting: The user select the dataset either production or consumption. Then selects target type (individual forecasting for each price area with each group or aggregated forecasting of selected multiple price areas with multiple groups). Then user select the remaining exogenous variables that are the groups. The training and forecast time frame is selected by user. All relevant SARIMAX parameters are also selected by user to make it more interactive. Finally, after forecasting a plot is rendered which displays the training data and forecasted data with the confidence interval."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deff9e18",
   "metadata": {},
   "source": [
    "**Sliding Window Correlation**\n",
    "\n",
    "- **(Consumption vs Temperature)** , Window unit: Hours , Window size: ~70 hours , Lag: ±4 hours   \n",
    "- Lag +4h: Correlation is stable and narrow in summer (June–Sept), but spread out in winter, showing stronger variability during cold events.  \n",
    "- Lag –4h: Pattern reverses — spread correlations in summer, while winter values cluster near zero, indicating weaker, steadier relationships.  \n",
    "- Lag direction changes the seasonal dynamics: positive lag highlights stable summer and volatile winter, while negative lag shows variable summer and flatter winter correlations.\n",
    "\n",
    "**(Consumption vs Precipitation)** :\n",
    "When analyzing consumption against precipitation, the sliding‑window correlation graph shows gaps.  \n",
    "This occurs because precipitation data often has long stretches of zeros (no rain).  \n",
    "During these periods, the variance of precipitation is zero, making correlation undefined.  \n",
    "The gaps highlight that meaningful correlation only emerges during rain events.  \n",
    "\n",
    "**(Consumption vs Wind/Wind Gusts/wind direction)**:\n",
    "- Winter months (Oct–Mar): \n",
    "  The correlation curve is less stable, with wider fluctuations and variability. This reflects the stronger influence of irregular wind patterns and extreme weather events on consumption.  \n",
    "\n",
    "- Summer months (Jun–Sept): \n",
    "  The correlation curve is relatively stable , indicating a steadier relationship between wind conditions and consumption during calmer seasonal weather.  \n",
    "\n",
    "Similarly trends can be explore for Production(kwh) vs weather properties."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b063cbee",
   "metadata": {},
   "source": [
    "## Jupyter Work\n",
    "- The following sections contain the jupyter tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c519689d",
   "metadata": {},
   "source": [
    "#### Data from API for Production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad9df7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Retrieving data for Production 2022 ===\n",
      " ✅ Added data for 2022-01-01 00:00:00+01:00 → 2022-02-01 00:00:00+01:00\n",
      " ✅ Added data for 2022-02-01 00:00:00+01:00 → 2022-03-01 00:00:00+01:00\n",
      " ✅ Added data for 2022-03-01 00:00:00+01:00 → 2022-04-01 00:00:00+02:00\n",
      " ✅ Added data for 2022-04-01 00:00:00+02:00 → 2022-05-01 00:00:00+02:00\n",
      " ✅ Added data for 2022-05-01 00:00:00+02:00 → 2022-06-01 00:00:00+02:00\n",
      " ✅ Added data for 2022-06-01 00:00:00+02:00 → 2022-07-01 00:00:00+02:00\n",
      " ✅ Added data for 2022-07-01 00:00:00+02:00 → 2022-08-01 00:00:00+02:00\n",
      " ✅ Added data for 2022-08-01 00:00:00+02:00 → 2022-09-01 00:00:00+02:00\n",
      " ✅ Added data for 2022-09-01 00:00:00+02:00 → 2022-10-01 00:00:00+02:00\n",
      " ✅ Added data for 2022-10-01 00:00:00+02:00 → 2022-10-30 01:00:00+02:00\n",
      " ✅ Added data for 2022-10-30 01:00:00+02:00 → 2022-11-01 00:00:00+01:00\n",
      " ✅ Added data for 2022-11-01 00:00:00+01:00 → 2022-12-01 00:00:00+01:00\n",
      " ✅ Added data for 2022-12-01 00:00:00+01:00 → 2023-01-01 00:00:00+01:00\n",
      " Total records collected for 2022: 219000\n",
      "\n",
      "=== Retrieving data for Production 2023 ===\n",
      " ✅ Added data for 2023-01-01 00:00:00+01:00 → 2023-02-01 00:00:00+01:00\n",
      " ✅ Added data for 2023-02-01 00:00:00+01:00 → 2023-03-01 00:00:00+01:00\n",
      " ✅ Added data for 2023-03-01 00:00:00+01:00 → 2023-04-01 00:00:00+02:00\n",
      " ✅ Added data for 2023-04-01 00:00:00+02:00 → 2023-05-01 00:00:00+02:00\n",
      " ✅ Added data for 2023-05-01 00:00:00+02:00 → 2023-06-01 00:00:00+02:00\n",
      " ✅ Added data for 2023-06-01 00:00:00+02:00 → 2023-07-01 00:00:00+02:00\n",
      " ✅ Added data for 2023-07-01 00:00:00+02:00 → 2023-08-01 00:00:00+02:00\n",
      " ✅ Added data for 2023-08-01 00:00:00+02:00 → 2023-09-01 00:00:00+02:00\n",
      " ✅ Added data for 2023-09-01 00:00:00+02:00 → 2023-10-01 00:00:00+02:00\n",
      " ✅ Added data for 2023-10-01 00:00:00+02:00 → 2023-10-29 01:00:00+02:00\n",
      " ✅ Added data for 2023-10-29 01:00:00+02:00 → 2023-11-01 00:00:00+01:00\n",
      " ✅ Added data for 2023-11-01 00:00:00+01:00 → 2023-12-01 00:00:00+01:00\n",
      " ✅ Added data for 2023-12-01 00:00:00+01:00 → 2024-01-01 00:00:00+01:00\n",
      " Total records collected for 2023: 219000\n",
      "\n",
      "=== Retrieving data for Production 2024 ===\n",
      " ✅ Added data for 2024-01-01 00:00:00+01:00 → 2024-02-01 00:00:00+01:00\n",
      " ✅ Added data for 2024-02-01 00:00:00+01:00 → 2024-03-01 00:00:00+01:00\n",
      " ✅ Added data for 2024-03-01 00:00:00+01:00 → 2024-04-01 00:00:00+02:00\n",
      " ✅ Added data for 2024-04-01 00:00:00+02:00 → 2024-05-01 00:00:00+02:00\n",
      " ✅ Added data for 2024-05-01 00:00:00+02:00 → 2024-06-01 00:00:00+02:00\n",
      " ✅ Added data for 2024-06-01 00:00:00+02:00 → 2024-07-01 00:00:00+02:00\n",
      " ✅ Added data for 2024-07-01 00:00:00+02:00 → 2024-08-01 00:00:00+02:00\n",
      " ✅ Added data for 2024-08-01 00:00:00+02:00 → 2024-09-01 00:00:00+02:00\n",
      " ✅ Added data for 2024-09-01 00:00:00+02:00 → 2024-10-01 00:00:00+02:00\n",
      " ✅ Added data for 2024-10-01 00:00:00+02:00 → 2024-10-27 01:00:00+02:00\n",
      " ✅ Added data for 2024-10-27 01:00:00+02:00 → 2024-11-01 00:00:00+01:00\n",
      " ✅ Added data for 2024-11-01 00:00:00+01:00 → 2024-12-01 00:00:00+01:00\n",
      " ✅ Added data for 2024-12-01 00:00:00+01:00 → 2025-01-01 00:00:00+01:00\n",
      " Total records collected for 2024: 219600\n",
      "\n",
      " Total records collected across all years: 657600\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from zoneinfo import ZoneInfo\n",
    "import requests\n",
    "\n",
    "#Defining to use for .get from the api\n",
    "url = \"https://api.elhub.no/energy-data/v0/price-areas\"\n",
    "dataset = \"PRODUCTION_PER_GROUP_MBA_HOUR\"\n",
    "# list of years to collect\n",
    "years = [2022, 2023, 2024]  # list of years to collect\n",
    "tz_norway = ZoneInfo(\"Europe/Oslo\")\n",
    "\n",
    "#Creating a function to get the dates for each month call for api\n",
    "def month_range(year):\n",
    "    for month in range(1, 13):\n",
    "        start_time = datetime(year, month, 1, tzinfo=tz_norway)\n",
    "        if month == 12:\n",
    "            end_time = datetime(year + 1, 1, 1, tzinfo=tz_norway)\n",
    "        else:\n",
    "            end_time = datetime(year, month + 1, 1, tzinfo=tz_norway)\n",
    "        yield start_time, end_time\n",
    "\n",
    "# last Sunday of October \n",
    "def last_sunday_of_october(year):\n",
    "    for day in range(31, 24, -1):  # 31 → 25\n",
    "        d = datetime(year, 10, day, tzinfo=tz_norway)\n",
    "        if d.weekday() == 6:  # Sunday\n",
    "            return d\n",
    "\n",
    "##Defining an empty list in which data will be stored from api\n",
    "production_data = []\n",
    "\n",
    "#Loop for list of years\n",
    "for year in years:\n",
    "    print(f\"\\n=== Retrieving data for Production {year} ===\")\n",
    "    year_count = 0\n",
    "    #Defining a loop to pass dates for each month call to api\n",
    "    for start_time, end_time in month_range(year):\n",
    "        # Split October due to DST change\n",
    "        if start_time.month == 10:\n",
    "            dst_shift = last_sunday_of_october(year).replace(hour=1)\n",
    "            parts = [(start_time, dst_shift), (dst_shift, end_time)]\n",
    "        else:\n",
    "            parts = [(start_time, end_time)]\n",
    "\n",
    "        # Requesting data for each part\n",
    "        for s_time, e_time in parts:\n",
    "            params = {\n",
    "                \"dataset\": dataset,\n",
    "                \"startDate\": s_time.isoformat(timespec=\"seconds\"),\n",
    "                \"endDate\": e_time.isoformat(timespec=\"seconds\")\n",
    "            }\n",
    "\n",
    "            response = requests.get(url, params=params)\n",
    "            if response.status_code == 200:\n",
    "                data_per_request = response.json()\n",
    "                # Looping over all areas in 'data' and extend production_data with their records \n",
    "                for area_data in data_per_request.get(\"data\", []):\n",
    "                    production_list = area_data.get(\"attributes\", {}).get(\"productionPerGroupMbaHour\", [])\n",
    "                    production_data.extend(production_list)\n",
    "                    year_count += len(production_list)\n",
    "                print(f\" ✅ Added data for {s_time} → {e_time}\")\n",
    "            else:\n",
    "                print(f\" Error {response.status_code} for {s_time} → {e_time}\")\n",
    "\n",
    "    print(f\" Total records collected for {year}: {year_count}\")\n",
    "\n",
    "print(f\"\\n Total records collected across all years: {len(production_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac599068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    endTime           lastUpdatedTime priceArea  \\\n",
      "0 2022-01-01 00:00:00+00:00 2025-02-01 17:02:57+00:00       NO1   \n",
      "1 2022-01-01 01:00:00+00:00 2025-02-01 17:02:57+00:00       NO1   \n",
      "2 2022-01-01 02:00:00+00:00 2025-02-01 17:02:57+00:00       NO1   \n",
      "3 2022-01-01 03:00:00+00:00 2025-02-01 17:02:57+00:00       NO1   \n",
      "4 2022-01-01 04:00:00+00:00 2025-02-01 17:02:57+00:00       NO1   \n",
      "\n",
      "  productionGroup  quantityKwh                 startTime  \n",
      "0           hydro    1291422.4 2021-12-31 23:00:00+00:00  \n",
      "1           hydro    1246209.4 2022-01-01 00:00:00+00:00  \n",
      "2           hydro    1271757.0 2022-01-01 01:00:00+00:00  \n",
      "3           hydro    1204251.8 2022-01-01 02:00:00+00:00  \n",
      "4           hydro    1202086.9 2022-01-01 03:00:00+00:00  \n",
      "                         endTime           lastUpdatedTime priceArea  \\\n",
      "657595 2024-12-31 19:00:00+00:00 2025-03-30 16:39:27+00:00       NO5   \n",
      "657596 2024-12-31 20:00:00+00:00 2025-03-30 16:39:27+00:00       NO5   \n",
      "657597 2024-12-31 21:00:00+00:00 2025-03-30 16:39:27+00:00       NO5   \n",
      "657598 2024-12-31 22:00:00+00:00 2025-03-30 16:39:27+00:00       NO5   \n",
      "657599 2024-12-31 23:00:00+00:00 2025-03-30 16:39:27+00:00       NO5   \n",
      "\n",
      "       productionGroup  quantityKwh                 startTime  \n",
      "657595            wind          0.0 2024-12-31 18:00:00+00:00  \n",
      "657596            wind          0.0 2024-12-31 19:00:00+00:00  \n",
      "657597            wind          0.0 2024-12-31 20:00:00+00:00  \n",
      "657598            wind          0.0 2024-12-31 21:00:00+00:00  \n",
      "657599            wind          0.0 2024-12-31 22:00:00+00:00  \n",
      "num rows 657600\n"
     ]
    }
   ],
   "source": [
    "# Creating a pandas dataframe\n",
    "import pandas as pd\n",
    "production_df=pd.DataFrame(production_data)\n",
    "\n",
    "# Converting startTime, endTime and lastUpdatedTime to datetime and setting the timezone to UTC \n",
    "production_df['startTime'] = pd.to_datetime(production_df['startTime'], utc=True)\n",
    "production_df['endTime'] = pd.to_datetime(production_df['endTime'], utc=True)\n",
    "production_df['lastUpdatedTime'] = pd.to_datetime(production_df['lastUpdatedTime'], utc=True)\n",
    "#Displaying few rows to verify \n",
    "print(production_df.head())\n",
    "print(production_df.tail())\n",
    "num_rows = len(production_df)\n",
    "print('num rows',num_rows)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f577ae29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cassandra setup \n",
    "from cassandra.cluster import Cluster\n",
    "cluster = Cluster(['localhost'], port=9042)\n",
    "session = cluster.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "755fd0c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session created successfully: 3.5.1\n"
     ]
    }
   ],
   "source": [
    "#Creating a Spark Cassandra Session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('Ind_320App').\\\n",
    "    config('spark.jars.packages', 'com.datastax.spark:spark-cassandra-connector_2.12:3.5.1').\\\n",
    "    config('spark.cassandra.connection.host', 'localhost').\\\n",
    "    config('spark.sql.extensions', 'com.datastax.spark.connector.CassandraSparkExtensions').\\\n",
    "    config('spark.sql.catalog.mycatalog', 'com.datastax.spark.connector.datasource.CassandraCatalog').\\\n",
    "    config('spark.cassandra.connection.port', '9042').getOrCreate()\n",
    "print(\"Spark session created successfully:\", spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2c2e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- endTime: timestamp (nullable = true)\n",
      " |-- lastUpdatedTime: timestamp (nullable = true)\n",
      " |-- priceArea: string (nullable = true)\n",
      " |-- productionGroup: string (nullable = true)\n",
      " |-- quantityKwh: double (nullable = true)\n",
      " |-- startTime: timestamp (nullable = true)\n",
      "\n",
      "+-------------------+-------------------+---------+---------------+-----------+-------------------+\n",
      "|            endTime|    lastUpdatedTime|priceArea|productionGroup|quantityKwh|          startTime|\n",
      "+-------------------+-------------------+---------+---------------+-----------+-------------------+\n",
      "|2022-01-01 01:00:00|2025-02-01 18:02:57|      NO1|          hydro|  1291422.4|2022-01-01 00:00:00|\n",
      "|2022-01-01 02:00:00|2025-02-01 18:02:57|      NO1|          hydro|  1246209.4|2022-01-01 01:00:00|\n",
      "|2022-01-01 03:00:00|2025-02-01 18:02:57|      NO1|          hydro|  1271757.0|2022-01-01 02:00:00|\n",
      "|2022-01-01 04:00:00|2025-02-01 18:02:57|      NO1|          hydro|  1204251.8|2022-01-01 03:00:00|\n",
      "|2022-01-01 05:00:00|2025-02-01 18:02:57|      NO1|          hydro|  1202086.9|2022-01-01 04:00:00|\n",
      "+-------------------+-------------------+---------+---------------+-----------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Number of rows: 657600\n"
     ]
    }
   ],
   "source": [
    "#Converting to spark dataframe from pandas dataframe\n",
    "spark_production_df = spark.createDataFrame(production_df)\n",
    "\n",
    "#Displaying info about columns from Spark DataFrame to verify\n",
    "spark_production_df.printSchema()\n",
    "spark_production_df.show(5)\n",
    "num_rows = spark_production_df.count()\n",
    "print(f\"Number of rows: {num_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3b3685",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Appending spark dataframe to cassandra, using .write to give data to cassandra\n",
    "spark_production_df.write.format(\"org.apache.spark.sql.cassandra\")\\\n",
    ".options(table=\"production_table\", keyspace=\"ind_320_d2\").mode(\"append\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47df3738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------+-------------------+-----------+\n",
      "|priceArea|productionGroup|          startTime|quantityKwh|\n",
      "+---------+---------------+-------------------+-----------+\n",
      "|      NO4|          other|2021-01-01 00:00:00|      0.161|\n",
      "|      NO4|          other|2021-01-01 01:00:00|      0.161|\n",
      "|      NO4|          other|2021-01-01 02:00:00|      0.161|\n",
      "|      NO4|          other|2021-01-01 03:00:00|      0.161|\n",
      "|      NO4|          other|2021-01-01 04:00:00|      0.161|\n",
      "+---------+---------------+-------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Data extraction from Cassandra into a Spark Dataframe, using .load() to load data from Cassandra as a Spark DataFrame.\n",
    "extracted_df_prod=spark.read.format(\"org.apache.spark.sql.cassandra\").options(table=\"production_table\", keyspace=\"ind_320_d2\").load().select(\"priceArea\", \"productionGroup\", \"startTime\",\"quantityKwh\")\n",
    "extracted_df_prod.show(5) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62eb2368",
   "metadata": {},
   "source": [
    "#### Data from API for Consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edd299b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Retrieving data for Consumption 2021 ===\n",
      " ✅ Added data for 2021-01-01 00:00:00+01:00 → 2021-02-01 00:00:00+01:00\n",
      " ✅ Added data for 2021-02-01 00:00:00+01:00 → 2021-03-01 00:00:00+01:00\n",
      " ✅ Added data for 2021-03-01 00:00:00+01:00 → 2021-04-01 00:00:00+02:00\n",
      " ✅ Added data for 2021-04-01 00:00:00+02:00 → 2021-05-01 00:00:00+02:00\n",
      " ✅ Added data for 2021-05-01 00:00:00+02:00 → 2021-06-01 00:00:00+02:00\n",
      " ✅ Added data for 2021-06-01 00:00:00+02:00 → 2021-07-01 00:00:00+02:00\n",
      " ✅ Added data for 2021-07-01 00:00:00+02:00 → 2021-08-01 00:00:00+02:00\n",
      " ✅ Added data for 2021-08-01 00:00:00+02:00 → 2021-09-01 00:00:00+02:00\n",
      " ✅ Added data for 2021-09-01 00:00:00+02:00 → 2021-10-01 00:00:00+02:00\n",
      " ✅ Added data for 2021-10-01 00:00:00+02:00 → 2021-10-31 01:00:00+02:00\n",
      " ✅ Added data for 2021-10-31 01:00:00+02:00 → 2021-11-01 00:00:00+01:00\n",
      " ✅ Added data for 2021-11-01 00:00:00+01:00 → 2021-12-01 00:00:00+01:00\n",
      " ✅ Added data for 2021-12-01 00:00:00+01:00 → 2022-01-01 00:00:00+01:00\n",
      " Total records collected for 2021: 219000\n",
      "\n",
      "=== Retrieving data for Consumption 2022 ===\n",
      " ✅ Added data for 2022-01-01 00:00:00+01:00 → 2022-02-01 00:00:00+01:00\n",
      " ✅ Added data for 2022-02-01 00:00:00+01:00 → 2022-03-01 00:00:00+01:00\n",
      " ✅ Added data for 2022-03-01 00:00:00+01:00 → 2022-04-01 00:00:00+02:00\n",
      " ✅ Added data for 2022-04-01 00:00:00+02:00 → 2022-05-01 00:00:00+02:00\n",
      " ✅ Added data for 2022-05-01 00:00:00+02:00 → 2022-06-01 00:00:00+02:00\n",
      " ✅ Added data for 2022-06-01 00:00:00+02:00 → 2022-07-01 00:00:00+02:00\n",
      " ✅ Added data for 2022-07-01 00:00:00+02:00 → 2022-08-01 00:00:00+02:00\n",
      " ✅ Added data for 2022-08-01 00:00:00+02:00 → 2022-09-01 00:00:00+02:00\n",
      " ✅ Added data for 2022-09-01 00:00:00+02:00 → 2022-10-01 00:00:00+02:00\n",
      " ✅ Added data for 2022-10-01 00:00:00+02:00 → 2022-10-30 01:00:00+02:00\n",
      " ✅ Added data for 2022-10-30 01:00:00+02:00 → 2022-11-01 00:00:00+01:00\n",
      " ✅ Added data for 2022-11-01 00:00:00+01:00 → 2022-12-01 00:00:00+01:00\n",
      " ✅ Added data for 2022-12-01 00:00:00+01:00 → 2023-01-01 00:00:00+01:00\n",
      " Total records collected for 2022: 219000\n",
      "\n",
      "=== Retrieving data for Consumption 2023 ===\n",
      " ✅ Added data for 2023-01-01 00:00:00+01:00 → 2023-02-01 00:00:00+01:00\n",
      " ✅ Added data for 2023-02-01 00:00:00+01:00 → 2023-03-01 00:00:00+01:00\n",
      " ✅ Added data for 2023-03-01 00:00:00+01:00 → 2023-04-01 00:00:00+02:00\n",
      " ✅ Added data for 2023-04-01 00:00:00+02:00 → 2023-05-01 00:00:00+02:00\n",
      " ✅ Added data for 2023-05-01 00:00:00+02:00 → 2023-06-01 00:00:00+02:00\n",
      " ✅ Added data for 2023-06-01 00:00:00+02:00 → 2023-07-01 00:00:00+02:00\n",
      " ✅ Added data for 2023-07-01 00:00:00+02:00 → 2023-08-01 00:00:00+02:00\n",
      " ✅ Added data for 2023-08-01 00:00:00+02:00 → 2023-09-01 00:00:00+02:00\n",
      " ✅ Added data for 2023-09-01 00:00:00+02:00 → 2023-10-01 00:00:00+02:00\n",
      " ✅ Added data for 2023-10-01 00:00:00+02:00 → 2023-10-29 01:00:00+02:00\n",
      " ✅ Added data for 2023-10-29 01:00:00+02:00 → 2023-11-01 00:00:00+01:00\n",
      " ✅ Added data for 2023-11-01 00:00:00+01:00 → 2023-12-01 00:00:00+01:00\n",
      " ✅ Added data for 2023-12-01 00:00:00+01:00 → 2024-01-01 00:00:00+01:00\n",
      " Total records collected for 2023: 219000\n",
      "\n",
      "=== Retrieving data for Consumption 2024 ===\n",
      " ✅ Added data for 2024-01-01 00:00:00+01:00 → 2024-02-01 00:00:00+01:00\n",
      " ✅ Added data for 2024-02-01 00:00:00+01:00 → 2024-03-01 00:00:00+01:00\n",
      " ✅ Added data for 2024-03-01 00:00:00+01:00 → 2024-04-01 00:00:00+02:00\n",
      " ✅ Added data for 2024-04-01 00:00:00+02:00 → 2024-05-01 00:00:00+02:00\n",
      " ✅ Added data for 2024-05-01 00:00:00+02:00 → 2024-06-01 00:00:00+02:00\n",
      " ✅ Added data for 2024-06-01 00:00:00+02:00 → 2024-07-01 00:00:00+02:00\n",
      " ✅ Added data for 2024-07-01 00:00:00+02:00 → 2024-08-01 00:00:00+02:00\n",
      " ✅ Added data for 2024-08-01 00:00:00+02:00 → 2024-09-01 00:00:00+02:00\n",
      " ✅ Added data for 2024-09-01 00:00:00+02:00 → 2024-10-01 00:00:00+02:00\n",
      " ✅ Added data for 2024-10-01 00:00:00+02:00 → 2024-10-27 01:00:00+02:00\n",
      " ✅ Added data for 2024-10-27 01:00:00+02:00 → 2024-11-01 00:00:00+01:00\n",
      " ✅ Added data for 2024-11-01 00:00:00+01:00 → 2024-12-01 00:00:00+01:00\n",
      " ✅ Added data for 2024-12-01 00:00:00+01:00 → 2025-01-01 00:00:00+01:00\n",
      " Total records collected for 2024: 219600\n",
      "\n",
      " Total records collected across all years: 876600\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from zoneinfo import ZoneInfo\n",
    "import requests\n",
    "\n",
    "#Defining to use for .get from the api\n",
    "url = \"https://api.elhub.no/energy-data/v0/price-areas\"\n",
    "dataset = \"CONSUMPTION_PER_GROUP_MBA_HOUR\"\n",
    "# list of years to collect\n",
    "years = [2021,2022, 2023, 2024]  \n",
    "tz_norway = ZoneInfo(\"Europe/Oslo\")\n",
    "\n",
    "#Creating a function to get the dates for each month call for api\n",
    "def month_range(year):\n",
    "    for month in range(1, 13):\n",
    "        start_time = datetime(year, month, 1, tzinfo=tz_norway)\n",
    "        if month == 12:\n",
    "            end_time = datetime(year + 1, 1, 1, tzinfo=tz_norway)\n",
    "        else:\n",
    "            end_time = datetime(year, month + 1, 1, tzinfo=tz_norway)\n",
    "        yield start_time, end_time\n",
    "\n",
    "# last Sunday of October \n",
    "def last_sunday_of_october(year):\n",
    "    for day in range(31, 24, -1):  # 31 → 25\n",
    "        d = datetime(year, 10, day, tzinfo=tz_norway)\n",
    "        if d.weekday() == 6:  # Sunday\n",
    "            return d\n",
    "\n",
    "##Defining an empty list in which data will be stored from api\n",
    "consumption_data = []\n",
    "\n",
    "#Loop for list of years\n",
    "for year in years:\n",
    "    print(f\"\\n=== Retrieving data for Consumption {year} ===\")\n",
    "    year_count = 0\n",
    "    #Defining a loop to pass dates for each month call to api\n",
    "    for start_time, end_time in month_range(year):\n",
    "        # Split October due to DST change\n",
    "        if start_time.month == 10:\n",
    "            dst_shift = last_sunday_of_october(year).replace(hour=1)\n",
    "            parts = [(start_time, dst_shift), (dst_shift, end_time)]\n",
    "        else:\n",
    "            parts = [(start_time, end_time)]\n",
    "\n",
    "        # Requesting data for each part\n",
    "        for s_time, e_time in parts:\n",
    "            params = {\n",
    "                \"dataset\": dataset,\n",
    "                \"startDate\": s_time.isoformat(timespec=\"seconds\"),\n",
    "                \"endDate\": e_time.isoformat(timespec=\"seconds\")\n",
    "            }\n",
    "\n",
    "            response = requests.get(url, params=params)\n",
    "            if response.status_code == 200:\n",
    "                data_per_request = response.json()\n",
    "                # Looping over all areas in 'data' and extend consumption_data with their records \n",
    "                for area_data in data_per_request.get(\"data\", []):\n",
    "                    consumption_list = area_data.get(\"attributes\", {}).get(\"consumptionPerGroupMbaHour\", [])\n",
    "                    consumption_data.extend(consumption_list)\n",
    "                    year_count += len(consumption_list)\n",
    "                print(f\" ✅ Added data for {s_time} → {e_time}\")\n",
    "            else:\n",
    "                print(f\" Error {response.status_code} for {s_time} → {e_time}\")\n",
    "\n",
    "    print(f\" Total records collected for {year}: {year_count}\")\n",
    "\n",
    "print(f\"\\n Total records collected across all years: {len(consumption_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d151aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  consumptionGroup                   endTime           lastUpdatedTime  \\\n",
      "0            cabin 2021-01-01 00:00:00+00:00 2024-12-20 09:35:40+00:00   \n",
      "1            cabin 2021-01-01 01:00:00+00:00 2024-12-20 09:35:40+00:00   \n",
      "2            cabin 2021-01-01 02:00:00+00:00 2024-12-20 09:35:40+00:00   \n",
      "3            cabin 2021-01-01 03:00:00+00:00 2024-12-20 09:35:40+00:00   \n",
      "4            cabin 2021-01-01 04:00:00+00:00 2024-12-20 09:35:40+00:00   \n",
      "\n",
      "   meteringPointCount priceArea  quantityKwh                 startTime  \n",
      "0              100607       NO1    177071.56 2020-12-31 23:00:00+00:00  \n",
      "1              100607       NO1    171335.12 2021-01-01 00:00:00+00:00  \n",
      "2              100607       NO1    164912.02 2021-01-01 01:00:00+00:00  \n",
      "3              100607       NO1    160265.77 2021-01-01 02:00:00+00:00  \n",
      "4              100607       NO1    159828.69 2021-01-01 03:00:00+00:00  \n",
      "       consumptionGroup                   endTime           lastUpdatedTime  \\\n",
      "876595         tertiary 2024-12-31 19:00:00+00:00 2025-03-30 16:36:55+00:00   \n",
      "876596         tertiary 2024-12-31 20:00:00+00:00 2025-03-30 16:36:55+00:00   \n",
      "876597         tertiary 2024-12-31 21:00:00+00:00 2025-03-30 16:36:55+00:00   \n",
      "876598         tertiary 2024-12-31 22:00:00+00:00 2025-03-30 16:36:55+00:00   \n",
      "876599         tertiary 2024-12-31 23:00:00+00:00 2025-03-30 16:36:55+00:00   \n",
      "\n",
      "        meteringPointCount priceArea  quantityKwh                 startTime  \n",
      "876595               29584       NO5    373752.66 2024-12-31 18:00:00+00:00  \n",
      "876596               29584       NO5    360142.56 2024-12-31 19:00:00+00:00  \n",
      "876597               29584       NO5    349349.44 2024-12-31 20:00:00+00:00  \n",
      "876598               29584       NO5    332428.20 2024-12-31 21:00:00+00:00  \n",
      "876599               29584       NO5    321213.88 2024-12-31 22:00:00+00:00  \n"
     ]
    }
   ],
   "source": [
    "# Creating a pandas dataframe\n",
    "import pandas as pd\n",
    "consumption_df=pd.DataFrame(consumption_data)\n",
    "\n",
    "# Converting startTime, endTime and lastUpdatedTime to datetime and setting the timezone to UTC \n",
    "consumption_df['startTime'] = pd.to_datetime(consumption_df['startTime'], utc=True)\n",
    "consumption_df['endTime'] = pd.to_datetime(consumption_df['endTime'], utc=True)\n",
    "consumption_df['lastUpdatedTime'] = pd.to_datetime(consumption_df['lastUpdatedTime'], utc=True)\n",
    "#Displaying few rows to verify \n",
    "print(consumption_df.head())\n",
    "print(consumption_df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8734cf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cassandra setup \n",
    "import time\n",
    "from cassandra.cluster import Cluster\n",
    "cluster = Cluster(['localhost'], port=9042)\n",
    "session = cluster.connect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08cf0e66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cassandra.cluster.ResultSet at 0x1d6c348d310>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Setting up a cassandra keyspace  \n",
    "\n",
    "# Use the existing keyspace\n",
    "session.set_keyspace(\"ind_320_d2\")\n",
    "\n",
    "#Setting a table in the keyspace and ensuring it didnt exist before\n",
    "session.execute(\"DROP TABLE IF EXISTS ind_320_d2.consumption_table;\")\n",
    "\n",
    "#Making sure that column names are entact so using case sensitive format\n",
    "session.execute(\"CREATE TABLE IF NOT EXISTS consumption_table (\"\n",
    "                \"\\\"startTime\\\" timestamp, \"\n",
    "                \"\\\"endTime\\\" timestamp, \"\n",
    "                \"\\\"lastUpdatedTime\\\" timestamp, \"\n",
    "                \"\\\"priceArea\\\" text, \"\n",
    "                \"\\\"consumptionGroup\\\" text, \"\n",
    "                \"\\\"meteringPointCount\\\" int, \"\n",
    "                \"\\\"quantityKwh\\\" double, \"  \n",
    "                \"PRIMARY KEY ((\\\"priceArea\\\", \\\"consumptionGroup\\\"), \\\"startTime\\\")) \"\n",
    "                \"WITH CLUSTERING ORDER BY (\\\"startTime\\\" ASC);\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a97ffd46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session created successfully: 3.5.1\n"
     ]
    }
   ],
   "source": [
    "#Creating a Spark Cassandra Session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('Ind_320App').\\\n",
    "    config('spark.jars.packages', 'com.datastax.spark:spark-cassandra-connector_2.12:3.5.1').\\\n",
    "    config('spark.cassandra.connection.host', 'localhost').\\\n",
    "    config('spark.sql.extensions', 'com.datastax.spark.connector.CassandraSparkExtensions').\\\n",
    "    config('spark.sql.catalog.mycatalog', 'com.datastax.spark.connector.datasource.CassandraCatalog').\\\n",
    "    config('spark.cassandra.connection.port', '9042').getOrCreate()\n",
    "print(\"Spark session created successfully:\", spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57a60ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- consumptionGroup: string (nullable = true)\n",
      " |-- endTime: timestamp (nullable = true)\n",
      " |-- lastUpdatedTime: timestamp (nullable = true)\n",
      " |-- meteringPointCount: long (nullable = true)\n",
      " |-- priceArea: string (nullable = true)\n",
      " |-- quantityKwh: double (nullable = true)\n",
      " |-- startTime: timestamp (nullable = true)\n",
      "\n",
      "+----------------+-------------------+-------------------+------------------+---------+-----------+-------------------+\n",
      "|consumptionGroup|            endTime|    lastUpdatedTime|meteringPointCount|priceArea|quantityKwh|          startTime|\n",
      "+----------------+-------------------+-------------------+------------------+---------+-----------+-------------------+\n",
      "|           cabin|2021-01-01 01:00:00|2024-12-20 10:35:40|            100607|      NO1|  177071.56|2021-01-01 00:00:00|\n",
      "|           cabin|2021-01-01 02:00:00|2024-12-20 10:35:40|            100607|      NO1|  171335.12|2021-01-01 01:00:00|\n",
      "|           cabin|2021-01-01 03:00:00|2024-12-20 10:35:40|            100607|      NO1|  164912.02|2021-01-01 02:00:00|\n",
      "|           cabin|2021-01-01 04:00:00|2024-12-20 10:35:40|            100607|      NO1|  160265.77|2021-01-01 03:00:00|\n",
      "|           cabin|2021-01-01 05:00:00|2024-12-20 10:35:40|            100607|      NO1|  159828.69|2021-01-01 04:00:00|\n",
      "+----------------+-------------------+-------------------+------------------+---------+-----------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Converting to spark dataframe from pandas dataframe\n",
    "spark_consumption_df = spark.createDataFrame(consumption_df)\n",
    "\n",
    "#Displaying info about columns from Spark DataFrame to verify\n",
    "spark_consumption_df.printSchema()\n",
    "spark_consumption_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f203bdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inserting spark dataframe to cassandra, using .write to give data to cassandra\n",
    "spark_consumption_df.write.format(\"org.apache.spark.sql.cassandra\")\\\n",
    ".options(table=\"consumption_table\", keyspace=\"ind_320_d2\").mode(\"append\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9431afac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------------+-------------------+-------------------+-----------+------------------+\n",
      "|priceArea|consumptionGroup|          startTime|            endTime|quantityKwh|meteringPointCount|\n",
      "+---------+----------------+-------------------+-------------------+-----------+------------------+\n",
      "|      NO5|       secondary|2021-01-01 00:00:00|2021-01-01 01:00:00|  1094799.6|              5984|\n",
      "|      NO5|       secondary|2021-01-01 01:00:00|2021-01-01 02:00:00|  1099480.8|              5984|\n",
      "|      NO5|       secondary|2021-01-01 02:00:00|2021-01-01 03:00:00|  1054455.9|              5984|\n",
      "|      NO5|       secondary|2021-01-01 03:00:00|2021-01-01 04:00:00|  1049728.6|              5984|\n",
      "|      NO5|       secondary|2021-01-01 04:00:00|2021-01-01 05:00:00|  1099942.9|              5984|\n",
      "+---------+----------------+-------------------+-------------------+-----------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Data extraction from Cassandra into a Spark Dataframe, using .load() to load data from Cassandra as a Spark DataFrame.\n",
    "extracted_df_cons=spark.read.format(\"org.apache.spark.sql.cassandra\").options(table=\"consumption_table\", keyspace=\"ind_320_d2\").load().select(\"priceArea\", \"consumptionGroup\", \"startTime\",\"endTime\",\"quantityKwh\",\"meteringPointCount\")\n",
    "extracted_df_cons.show(5) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee258c14",
   "metadata": {},
   "source": [
    "### MongoDb Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ece55f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pinged your deployment. You successfully connected to MongoDB!\n"
     ]
    }
   ],
   "source": [
    "#Connecting to mongodb\n",
    "\n",
    "from pymongo.mongo_client import MongoClient\n",
    "from pymongo.server_api import ServerApi\n",
    "\n",
    "username,password = open(r\"D:\\NMBU\\semester_1\\IND-320\\mongodb_password.txt\").read().strip().split(',')\n",
    "\n",
    "uri = f\"mongodb+srv://{username}:{password}@app-cluster.ihj1zbx.mongodb.net/?retryWrites=true&w=majority&appName=app-cluster\"\n",
    "\n",
    "# Create a new client and connect to the server\n",
    "client = MongoClient(uri, server_api=ServerApi('1'))\n",
    "\n",
    "# Send a ping to confirm a successful connection\n",
    "try:\n",
    "    client.admin.command('ping')\n",
    "    print(\"Pinged your deployment. You successfully connected to MongoDB!\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c82bb288",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining a database and collection inside the cluster \n",
    "database=client['ind320_production_db']\n",
    "collection=database['ind320_production_table_d4']\n",
    "collection2=database['ind320_consumption_table']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ecc1e91a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark data inserted into MongoDB successfully!\n"
     ]
    }
   ],
   "source": [
    "#Converting Spark Dataframe to Pandas Dataframe and dictionaries (PRODUCTION DATA)\n",
    "pandas_df = extracted_df_prod.toPandas()\n",
    "data_dict = pandas_df.to_dict(\"records\")\n",
    "\n",
    "#Inserting into MongoDB\n",
    "collection.insert_many(data_dict)\n",
    "\n",
    "print(\"Spark data inserted into MongoDB successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eeb07703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark data inserted into MongoDB successfully!\n"
     ]
    }
   ],
   "source": [
    "#Converting Spark Dataframe to Pandas Dataframe and dictionaries(CONSUMPTION DATA)\n",
    "pandas_df_cons = extracted_df_cons.toPandas()\n",
    "data_dict_cons = pandas_df_cons.to_dict(\"records\")\n",
    "\n",
    "#Inserting into MongoDB\n",
    "collection2.insert_many(data_dict_cons)\n",
    "\n",
    "print(\"Spark data inserted into MongoDB successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ind320_try1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
