{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3d5aab0",
   "metadata": {},
   "source": [
    "# Project IND-320\n",
    "**Name** : _Izza Qamar_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a390258d",
   "metadata": {},
   "source": [
    "## Links\n",
    " - **GitHub Repository** : https://github.com/izzaqamar/Izza_Ind_320.git  \n",
    "\n",
    " - **Streamlit App** : https://izza-ind320.streamlit.app/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08fbb9d",
   "metadata": {},
   "source": [
    "# Deliverable 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5742801e",
   "metadata": {},
   "source": [
    "## Project Overview and AI Usage "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae84cad",
   "metadata": {},
   "source": [
    "### AI tools\n",
    "- For Jupyter task, I have used chatgpt to understand how I can extend my previous api_call function to multiplr years for production and  consumption.\n",
    "- For Streamlit, I have used both chatgpt and copilot. Initially to understand use of folium as I have never used it before and to add outlines and Choropleth to the map. Then to understand the how to make plot of wind rose and snow drift. Then for the forecasting part I used it to read about Sarimax and its parameters. I also took help to understand how I can incorpertate the confidence interval on my graph for forecasting.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408f47a0",
   "metadata": {},
   "source": [
    "### Project Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c26897",
   "metadata": {},
   "source": [
    "#### Jupter Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba87106",
   "metadata": {},
   "source": [
    "#### Streamlit app"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b063cbee",
   "metadata": {},
   "source": [
    "## Jupyter Work\n",
    "- The following sections contain the jupyter tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c519689d",
   "metadata": {},
   "source": [
    "#### Data from API for Production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ad9df7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Retrieving data for Production 2022 ===\n",
      " ✅ Added data for 2022-01-01 00:00:00+01:00 → 2022-02-01 00:00:00+01:00\n",
      " ✅ Added data for 2022-02-01 00:00:00+01:00 → 2022-03-01 00:00:00+01:00\n",
      " ✅ Added data for 2022-03-01 00:00:00+01:00 → 2022-04-01 00:00:00+02:00\n",
      " ✅ Added data for 2022-04-01 00:00:00+02:00 → 2022-05-01 00:00:00+02:00\n",
      " ✅ Added data for 2022-05-01 00:00:00+02:00 → 2022-06-01 00:00:00+02:00\n",
      " ✅ Added data for 2022-06-01 00:00:00+02:00 → 2022-07-01 00:00:00+02:00\n",
      " ✅ Added data for 2022-07-01 00:00:00+02:00 → 2022-08-01 00:00:00+02:00\n",
      " ✅ Added data for 2022-08-01 00:00:00+02:00 → 2022-09-01 00:00:00+02:00\n",
      " ✅ Added data for 2022-09-01 00:00:00+02:00 → 2022-10-01 00:00:00+02:00\n",
      " ✅ Added data for 2022-10-01 00:00:00+02:00 → 2022-10-30 01:00:00+02:00\n",
      " ✅ Added data for 2022-10-30 01:00:00+02:00 → 2022-11-01 00:00:00+01:00\n",
      " ✅ Added data for 2022-11-01 00:00:00+01:00 → 2022-12-01 00:00:00+01:00\n",
      " ✅ Added data for 2022-12-01 00:00:00+01:00 → 2023-01-01 00:00:00+01:00\n",
      " Total records collected for 2022: 219000\n",
      "\n",
      "=== Retrieving data for Production 2023 ===\n",
      " ✅ Added data for 2023-01-01 00:00:00+01:00 → 2023-02-01 00:00:00+01:00\n",
      " ✅ Added data for 2023-02-01 00:00:00+01:00 → 2023-03-01 00:00:00+01:00\n",
      " ✅ Added data for 2023-03-01 00:00:00+01:00 → 2023-04-01 00:00:00+02:00\n",
      " ✅ Added data for 2023-04-01 00:00:00+02:00 → 2023-05-01 00:00:00+02:00\n",
      " ✅ Added data for 2023-05-01 00:00:00+02:00 → 2023-06-01 00:00:00+02:00\n",
      " ✅ Added data for 2023-06-01 00:00:00+02:00 → 2023-07-01 00:00:00+02:00\n",
      " ✅ Added data for 2023-07-01 00:00:00+02:00 → 2023-08-01 00:00:00+02:00\n",
      " ✅ Added data for 2023-08-01 00:00:00+02:00 → 2023-09-01 00:00:00+02:00\n",
      " ✅ Added data for 2023-09-01 00:00:00+02:00 → 2023-10-01 00:00:00+02:00\n",
      " ✅ Added data for 2023-10-01 00:00:00+02:00 → 2023-10-29 01:00:00+02:00\n",
      " ✅ Added data for 2023-10-29 01:00:00+02:00 → 2023-11-01 00:00:00+01:00\n",
      " ✅ Added data for 2023-11-01 00:00:00+01:00 → 2023-12-01 00:00:00+01:00\n",
      " ✅ Added data for 2023-12-01 00:00:00+01:00 → 2024-01-01 00:00:00+01:00\n",
      " Total records collected for 2023: 219000\n",
      "\n",
      "=== Retrieving data for Production 2024 ===\n",
      " ✅ Added data for 2024-01-01 00:00:00+01:00 → 2024-02-01 00:00:00+01:00\n",
      " ✅ Added data for 2024-02-01 00:00:00+01:00 → 2024-03-01 00:00:00+01:00\n",
      " ✅ Added data for 2024-03-01 00:00:00+01:00 → 2024-04-01 00:00:00+02:00\n",
      " ✅ Added data for 2024-04-01 00:00:00+02:00 → 2024-05-01 00:00:00+02:00\n",
      " ✅ Added data for 2024-05-01 00:00:00+02:00 → 2024-06-01 00:00:00+02:00\n",
      " ✅ Added data for 2024-06-01 00:00:00+02:00 → 2024-07-01 00:00:00+02:00\n",
      " ✅ Added data for 2024-07-01 00:00:00+02:00 → 2024-08-01 00:00:00+02:00\n",
      " ✅ Added data for 2024-08-01 00:00:00+02:00 → 2024-09-01 00:00:00+02:00\n",
      " ✅ Added data for 2024-09-01 00:00:00+02:00 → 2024-10-01 00:00:00+02:00\n",
      " ✅ Added data for 2024-10-01 00:00:00+02:00 → 2024-10-27 01:00:00+02:00\n",
      " ✅ Added data for 2024-10-27 01:00:00+02:00 → 2024-11-01 00:00:00+01:00\n",
      " ✅ Added data for 2024-11-01 00:00:00+01:00 → 2024-12-01 00:00:00+01:00\n",
      " ✅ Added data for 2024-12-01 00:00:00+01:00 → 2025-01-01 00:00:00+01:00\n",
      " Total records collected for 2024: 219600\n",
      "\n",
      " Total records collected across all years: 657600\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from zoneinfo import ZoneInfo\n",
    "import requests\n",
    "\n",
    "#Defining to use for .get from the api\n",
    "url = \"https://api.elhub.no/energy-data/v0/price-areas\"\n",
    "dataset = \"PRODUCTION_PER_GROUP_MBA_HOUR\"\n",
    "# list of years to collect\n",
    "years = [2022, 2023, 2024]  # list of years to collect\n",
    "tz_norway = ZoneInfo(\"Europe/Oslo\")\n",
    "\n",
    "#Creating a function to get the dates for each month call for api\n",
    "def month_range(year):\n",
    "    for month in range(1, 13):\n",
    "        start_time = datetime(year, month, 1, tzinfo=tz_norway)\n",
    "        if month == 12:\n",
    "            end_time = datetime(year + 1, 1, 1, tzinfo=tz_norway)\n",
    "        else:\n",
    "            end_time = datetime(year, month + 1, 1, tzinfo=tz_norway)\n",
    "        yield start_time, end_time\n",
    "\n",
    "# last Sunday of October \n",
    "def last_sunday_of_october(year):\n",
    "    for day in range(31, 24, -1):  # 31 → 25\n",
    "        d = datetime(year, 10, day, tzinfo=tz_norway)\n",
    "        if d.weekday() == 6:  # Sunday\n",
    "            return d\n",
    "\n",
    "##Defining an empty list in which data will be stored from api\n",
    "consumption_data = []\n",
    "\n",
    "#Loop for list of years\n",
    "for year in years:\n",
    "    print(f\"\\n=== Retrieving data for Production {year} ===\")\n",
    "    year_count = 0\n",
    "    #Defining a loop to pass dates for each month call to api\n",
    "    for start_time, end_time in month_range(year):\n",
    "        # Split October due to DST change\n",
    "        if start_time.month == 10:\n",
    "            dst_shift = last_sunday_of_october(year).replace(hour=1)\n",
    "            parts = [(start_time, dst_shift), (dst_shift, end_time)]\n",
    "        else:\n",
    "            parts = [(start_time, end_time)]\n",
    "\n",
    "        # Requesting data for each part\n",
    "        for s_time, e_time in parts:\n",
    "            params = {\n",
    "                \"dataset\": dataset,\n",
    "                \"startDate\": s_time.isoformat(timespec=\"seconds\"),\n",
    "                \"endDate\": e_time.isoformat(timespec=\"seconds\")\n",
    "            }\n",
    "\n",
    "            response = requests.get(url, params=params)\n",
    "            if response.status_code == 200:\n",
    "                data_per_request = response.json()\n",
    "                # Looping over all areas in 'data' and extend production_data with their records \n",
    "                for area_data in data_per_request.get(\"data\", []):\n",
    "                    consumption_list = area_data.get(\"attributes\", {}).get(\"productionPerGroupMbaHour\", [])\n",
    "                    consumption_data.extend(consumption_list)\n",
    "                    year_count += len(consumption_list)\n",
    "                print(f\" ✅ Added data for {s_time} → {e_time}\")\n",
    "            else:\n",
    "                print(f\" Error {response.status_code} for {s_time} → {e_time}\")\n",
    "\n",
    "    print(f\" Total records collected for {year}: {year_count}\")\n",
    "\n",
    "print(f\"\\n Total records collected across all years: {len(consumption_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac599068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    endTime           lastUpdatedTime priceArea  \\\n",
      "0 2022-01-01 00:00:00+00:00 2025-02-01 17:02:57+00:00       NO1   \n",
      "1 2022-01-01 01:00:00+00:00 2025-02-01 17:02:57+00:00       NO1   \n",
      "2 2022-01-01 02:00:00+00:00 2025-02-01 17:02:57+00:00       NO1   \n",
      "3 2022-01-01 03:00:00+00:00 2025-02-01 17:02:57+00:00       NO1   \n",
      "4 2022-01-01 04:00:00+00:00 2025-02-01 17:02:57+00:00       NO1   \n",
      "\n",
      "  productionGroup  quantityKwh                 startTime  \n",
      "0           hydro    1291422.4 2021-12-31 23:00:00+00:00  \n",
      "1           hydro    1246209.4 2022-01-01 00:00:00+00:00  \n",
      "2           hydro    1271757.0 2022-01-01 01:00:00+00:00  \n",
      "3           hydro    1204251.8 2022-01-01 02:00:00+00:00  \n",
      "4           hydro    1202086.9 2022-01-01 03:00:00+00:00  \n",
      "                         endTime           lastUpdatedTime priceArea  \\\n",
      "657595 2024-12-31 19:00:00+00:00 2025-03-30 16:39:27+00:00       NO5   \n",
      "657596 2024-12-31 20:00:00+00:00 2025-03-30 16:39:27+00:00       NO5   \n",
      "657597 2024-12-31 21:00:00+00:00 2025-03-30 16:39:27+00:00       NO5   \n",
      "657598 2024-12-31 22:00:00+00:00 2025-03-30 16:39:27+00:00       NO5   \n",
      "657599 2024-12-31 23:00:00+00:00 2025-03-30 16:39:27+00:00       NO5   \n",
      "\n",
      "       productionGroup  quantityKwh                 startTime  \n",
      "657595            wind          0.0 2024-12-31 18:00:00+00:00  \n",
      "657596            wind          0.0 2024-12-31 19:00:00+00:00  \n",
      "657597            wind          0.0 2024-12-31 20:00:00+00:00  \n",
      "657598            wind          0.0 2024-12-31 21:00:00+00:00  \n",
      "657599            wind          0.0 2024-12-31 22:00:00+00:00  \n",
      "num rows 657600\n"
     ]
    }
   ],
   "source": [
    "# Creating a pandas dataframe\n",
    "import pandas as pd\n",
    "consumption_df=pd.DataFrame(consumption_data)\n",
    "\n",
    "# Converting startTime, endTime and lastUpdatedTime to datetime and setting the timezone to UTC \n",
    "consumption_df['startTime'] = pd.to_datetime(consumption_df['startTime'], utc=True)\n",
    "consumption_df['endTime'] = pd.to_datetime(consumption_df['endTime'], utc=True)\n",
    "consumption_df['lastUpdatedTime'] = pd.to_datetime(consumption_df['lastUpdatedTime'], utc=True)\n",
    "#Displaying few rows to verify \n",
    "print(consumption_df.head())\n",
    "print(consumption_df.tail())\n",
    "num_rows = len(consumption_df)\n",
    "print('num rows',num_rows)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f577ae29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cassandra setup \n",
    "from cassandra.cluster import Cluster\n",
    "cluster = Cluster(['localhost'], port=9042)\n",
    "session = cluster.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "755fd0c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session created successfully: 3.5.1\n"
     ]
    }
   ],
   "source": [
    "#Creating a Spark Cassandra Session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('Ind_320App').\\\n",
    "    config('spark.jars.packages', 'com.datastax.spark:spark-cassandra-connector_2.12:3.5.1').\\\n",
    "    config('spark.cassandra.connection.host', 'localhost').\\\n",
    "    config('spark.sql.extensions', 'com.datastax.spark.connector.CassandraSparkExtensions').\\\n",
    "    config('spark.sql.catalog.mycatalog', 'com.datastax.spark.connector.datasource.CassandraCatalog').\\\n",
    "    config('spark.cassandra.connection.port', '9042').getOrCreate()\n",
    "print(\"Spark session created successfully:\", spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f2c2e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- endTime: timestamp (nullable = true)\n",
      " |-- lastUpdatedTime: timestamp (nullable = true)\n",
      " |-- priceArea: string (nullable = true)\n",
      " |-- productionGroup: string (nullable = true)\n",
      " |-- quantityKwh: double (nullable = true)\n",
      " |-- startTime: timestamp (nullable = true)\n",
      "\n",
      "+-------------------+-------------------+---------+---------------+-----------+-------------------+\n",
      "|            endTime|    lastUpdatedTime|priceArea|productionGroup|quantityKwh|          startTime|\n",
      "+-------------------+-------------------+---------+---------------+-----------+-------------------+\n",
      "|2022-01-01 01:00:00|2025-02-01 18:02:57|      NO1|          hydro|  1291422.4|2022-01-01 00:00:00|\n",
      "|2022-01-01 02:00:00|2025-02-01 18:02:57|      NO1|          hydro|  1246209.4|2022-01-01 01:00:00|\n",
      "|2022-01-01 03:00:00|2025-02-01 18:02:57|      NO1|          hydro|  1271757.0|2022-01-01 02:00:00|\n",
      "|2022-01-01 04:00:00|2025-02-01 18:02:57|      NO1|          hydro|  1204251.8|2022-01-01 03:00:00|\n",
      "|2022-01-01 05:00:00|2025-02-01 18:02:57|      NO1|          hydro|  1202086.9|2022-01-01 04:00:00|\n",
      "+-------------------+-------------------+---------+---------------+-----------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Number of rows: 657600\n"
     ]
    }
   ],
   "source": [
    "#Converting to spark dataframe from pandas dataframe\n",
    "spark_consumption_df = spark.createDataFrame(consumption_df)\n",
    "\n",
    "#Displaying info about columns from Spark DataFrame to verify\n",
    "spark_consumption_df.printSchema()\n",
    "spark_consumption_df.show(5)\n",
    "num_rows = spark_consumption_df.count()\n",
    "print(f\"Number of rows: {num_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a3b3685",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Appending spark dataframe to cassandra, using .write to give data to cassandra\n",
    "spark_consumption_df.write.format(\"org.apache.spark.sql.cassandra\")\\\n",
    ".options(table=\"production_table\", keyspace=\"ind_320_d2\").mode(\"append\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47df3738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------+-------------------+-----------+\n",
      "|priceArea|productionGroup|          startTime|quantityKwh|\n",
      "+---------+---------------+-------------------+-----------+\n",
      "|      NO4|          other|2021-01-01 00:00:00|      0.161|\n",
      "|      NO4|          other|2021-01-01 01:00:00|      0.161|\n",
      "|      NO4|          other|2021-01-01 02:00:00|      0.161|\n",
      "|      NO4|          other|2021-01-01 03:00:00|      0.161|\n",
      "|      NO4|          other|2021-01-01 04:00:00|      0.161|\n",
      "+---------+---------------+-------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Data extraction from Cassandra into a Spark Dataframe, using .load() to load data from Cassandra as a Spark DataFrame.\n",
    "extracted_df_prod=spark.read.format(\"org.apache.spark.sql.cassandra\").options(table=\"production_table\", keyspace=\"ind_320_d2\").load().select(\"priceArea\", \"productionGroup\", \"startTime\",\"quantityKwh\")\n",
    "extracted_df_prod.show(5) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62eb2368",
   "metadata": {},
   "source": [
    "#### Data from API for Consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5edd299b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Retrieving data for Consumption 2021 ===\n",
      " ✅ Added data for 2021-01-01 00:00:00+01:00 → 2021-02-01 00:00:00+01:00\n",
      " ✅ Added data for 2021-02-01 00:00:00+01:00 → 2021-03-01 00:00:00+01:00\n",
      " ✅ Added data for 2021-03-01 00:00:00+01:00 → 2021-04-01 00:00:00+02:00\n",
      " ✅ Added data for 2021-04-01 00:00:00+02:00 → 2021-05-01 00:00:00+02:00\n",
      " ✅ Added data for 2021-05-01 00:00:00+02:00 → 2021-06-01 00:00:00+02:00\n",
      " ✅ Added data for 2021-06-01 00:00:00+02:00 → 2021-07-01 00:00:00+02:00\n",
      " ✅ Added data for 2021-07-01 00:00:00+02:00 → 2021-08-01 00:00:00+02:00\n",
      " ✅ Added data for 2021-08-01 00:00:00+02:00 → 2021-09-01 00:00:00+02:00\n",
      " ✅ Added data for 2021-09-01 00:00:00+02:00 → 2021-10-01 00:00:00+02:00\n",
      " ✅ Added data for 2021-10-01 00:00:00+02:00 → 2021-10-31 01:00:00+02:00\n",
      " ✅ Added data for 2021-10-31 01:00:00+02:00 → 2021-11-01 00:00:00+01:00\n",
      " ✅ Added data for 2021-11-01 00:00:00+01:00 → 2021-12-01 00:00:00+01:00\n",
      " ✅ Added data for 2021-12-01 00:00:00+01:00 → 2022-01-01 00:00:00+01:00\n",
      " Total records collected for 2021: 219000\n",
      "\n",
      "=== Retrieving data for Consumption 2022 ===\n",
      " ✅ Added data for 2022-01-01 00:00:00+01:00 → 2022-02-01 00:00:00+01:00\n",
      " ✅ Added data for 2022-02-01 00:00:00+01:00 → 2022-03-01 00:00:00+01:00\n",
      " ✅ Added data for 2022-03-01 00:00:00+01:00 → 2022-04-01 00:00:00+02:00\n",
      " ✅ Added data for 2022-04-01 00:00:00+02:00 → 2022-05-01 00:00:00+02:00\n",
      " ✅ Added data for 2022-05-01 00:00:00+02:00 → 2022-06-01 00:00:00+02:00\n",
      " ✅ Added data for 2022-06-01 00:00:00+02:00 → 2022-07-01 00:00:00+02:00\n",
      " ✅ Added data for 2022-07-01 00:00:00+02:00 → 2022-08-01 00:00:00+02:00\n",
      " ✅ Added data for 2022-08-01 00:00:00+02:00 → 2022-09-01 00:00:00+02:00\n",
      " ✅ Added data for 2022-09-01 00:00:00+02:00 → 2022-10-01 00:00:00+02:00\n",
      " ✅ Added data for 2022-10-01 00:00:00+02:00 → 2022-10-30 01:00:00+02:00\n",
      " ✅ Added data for 2022-10-30 01:00:00+02:00 → 2022-11-01 00:00:00+01:00\n",
      " ✅ Added data for 2022-11-01 00:00:00+01:00 → 2022-12-01 00:00:00+01:00\n",
      " ✅ Added data for 2022-12-01 00:00:00+01:00 → 2023-01-01 00:00:00+01:00\n",
      " Total records collected for 2022: 219000\n",
      "\n",
      "=== Retrieving data for Consumption 2023 ===\n",
      " ✅ Added data for 2023-01-01 00:00:00+01:00 → 2023-02-01 00:00:00+01:00\n",
      " ✅ Added data for 2023-02-01 00:00:00+01:00 → 2023-03-01 00:00:00+01:00\n",
      " ✅ Added data for 2023-03-01 00:00:00+01:00 → 2023-04-01 00:00:00+02:00\n",
      " ✅ Added data for 2023-04-01 00:00:00+02:00 → 2023-05-01 00:00:00+02:00\n",
      " ✅ Added data for 2023-05-01 00:00:00+02:00 → 2023-06-01 00:00:00+02:00\n",
      " ✅ Added data for 2023-06-01 00:00:00+02:00 → 2023-07-01 00:00:00+02:00\n",
      " ✅ Added data for 2023-07-01 00:00:00+02:00 → 2023-08-01 00:00:00+02:00\n",
      " ✅ Added data for 2023-08-01 00:00:00+02:00 → 2023-09-01 00:00:00+02:00\n",
      " ✅ Added data for 2023-09-01 00:00:00+02:00 → 2023-10-01 00:00:00+02:00\n",
      " ✅ Added data for 2023-10-01 00:00:00+02:00 → 2023-10-29 01:00:00+02:00\n",
      " ✅ Added data for 2023-10-29 01:00:00+02:00 → 2023-11-01 00:00:00+01:00\n",
      " ✅ Added data for 2023-11-01 00:00:00+01:00 → 2023-12-01 00:00:00+01:00\n",
      " ✅ Added data for 2023-12-01 00:00:00+01:00 → 2024-01-01 00:00:00+01:00\n",
      " Total records collected for 2023: 219000\n",
      "\n",
      "=== Retrieving data for Consumption 2024 ===\n",
      " ✅ Added data for 2024-01-01 00:00:00+01:00 → 2024-02-01 00:00:00+01:00\n",
      " ✅ Added data for 2024-02-01 00:00:00+01:00 → 2024-03-01 00:00:00+01:00\n",
      " ✅ Added data for 2024-03-01 00:00:00+01:00 → 2024-04-01 00:00:00+02:00\n",
      " ✅ Added data for 2024-04-01 00:00:00+02:00 → 2024-05-01 00:00:00+02:00\n",
      " ✅ Added data for 2024-05-01 00:00:00+02:00 → 2024-06-01 00:00:00+02:00\n",
      " ✅ Added data for 2024-06-01 00:00:00+02:00 → 2024-07-01 00:00:00+02:00\n",
      " ✅ Added data for 2024-07-01 00:00:00+02:00 → 2024-08-01 00:00:00+02:00\n",
      " ✅ Added data for 2024-08-01 00:00:00+02:00 → 2024-09-01 00:00:00+02:00\n",
      " ✅ Added data for 2024-09-01 00:00:00+02:00 → 2024-10-01 00:00:00+02:00\n",
      " ✅ Added data for 2024-10-01 00:00:00+02:00 → 2024-10-27 01:00:00+02:00\n",
      " ✅ Added data for 2024-10-27 01:00:00+02:00 → 2024-11-01 00:00:00+01:00\n",
      " ✅ Added data for 2024-11-01 00:00:00+01:00 → 2024-12-01 00:00:00+01:00\n",
      " ✅ Added data for 2024-12-01 00:00:00+01:00 → 2025-01-01 00:00:00+01:00\n",
      " Total records collected for 2024: 219600\n",
      "\n",
      " Total records collected across all years: 876600\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from zoneinfo import ZoneInfo\n",
    "import requests\n",
    "\n",
    "#Defining to use for .get from the api\n",
    "url = \"https://api.elhub.no/energy-data/v0/price-areas\"\n",
    "dataset = \"CONSUMPTION_PER_GROUP_MBA_HOUR\"\n",
    "# list of years to collect\n",
    "years = [2021,2022, 2023, 2024]  \n",
    "tz_norway = ZoneInfo(\"Europe/Oslo\")\n",
    "\n",
    "#Creating a function to get the dates for each month call for api\n",
    "def month_range(year):\n",
    "    for month in range(1, 13):\n",
    "        start_time = datetime(year, month, 1, tzinfo=tz_norway)\n",
    "        if month == 12:\n",
    "            end_time = datetime(year + 1, 1, 1, tzinfo=tz_norway)\n",
    "        else:\n",
    "            end_time = datetime(year, month + 1, 1, tzinfo=tz_norway)\n",
    "        yield start_time, end_time\n",
    "\n",
    "# last Sunday of October \n",
    "def last_sunday_of_october(year):\n",
    "    for day in range(31, 24, -1):  # 31 → 25\n",
    "        d = datetime(year, 10, day, tzinfo=tz_norway)\n",
    "        if d.weekday() == 6:  # Sunday\n",
    "            return d\n",
    "\n",
    "##Defining an empty list in which data will be stored from api\n",
    "consumption_data = []\n",
    "\n",
    "#Loop for list of years\n",
    "for year in years:\n",
    "    print(f\"\\n=== Retrieving data for Consumption {year} ===\")\n",
    "    year_count = 0\n",
    "    #Defining a loop to pass dates for each month call to api\n",
    "    for start_time, end_time in month_range(year):\n",
    "        # Split October due to DST change\n",
    "        if start_time.month == 10:\n",
    "            dst_shift = last_sunday_of_october(year).replace(hour=1)\n",
    "            parts = [(start_time, dst_shift), (dst_shift, end_time)]\n",
    "        else:\n",
    "            parts = [(start_time, end_time)]\n",
    "\n",
    "        # Requesting data for each part\n",
    "        for s_time, e_time in parts:\n",
    "            params = {\n",
    "                \"dataset\": dataset,\n",
    "                \"startDate\": s_time.isoformat(timespec=\"seconds\"),\n",
    "                \"endDate\": e_time.isoformat(timespec=\"seconds\")\n",
    "            }\n",
    "\n",
    "            response = requests.get(url, params=params)\n",
    "            if response.status_code == 200:\n",
    "                data_per_request = response.json()\n",
    "                # Looping over all areas in 'data' and extend production_data with their records \n",
    "                for area_data in data_per_request.get(\"data\", []):\n",
    "                    consumption_list = area_data.get(\"attributes\", {}).get(\"consumptionPerGroupMbaHour\", [])\n",
    "                    consumption_data.extend(consumption_list)\n",
    "                    year_count += len(consumption_list)\n",
    "                print(f\" ✅ Added data for {s_time} → {e_time}\")\n",
    "            else:\n",
    "                print(f\" Error {response.status_code} for {s_time} → {e_time}\")\n",
    "\n",
    "    print(f\" Total records collected for {year}: {year_count}\")\n",
    "\n",
    "print(f\"\\n Total records collected across all years: {len(consumption_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d151aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  consumptionGroup                   endTime           lastUpdatedTime  \\\n",
      "0            cabin 2021-01-01 00:00:00+00:00 2024-12-20 09:35:40+00:00   \n",
      "1            cabin 2021-01-01 01:00:00+00:00 2024-12-20 09:35:40+00:00   \n",
      "2            cabin 2021-01-01 02:00:00+00:00 2024-12-20 09:35:40+00:00   \n",
      "3            cabin 2021-01-01 03:00:00+00:00 2024-12-20 09:35:40+00:00   \n",
      "4            cabin 2021-01-01 04:00:00+00:00 2024-12-20 09:35:40+00:00   \n",
      "\n",
      "   meteringPointCount priceArea  quantityKwh                 startTime  \n",
      "0              100607       NO1    177071.56 2020-12-31 23:00:00+00:00  \n",
      "1              100607       NO1    171335.12 2021-01-01 00:00:00+00:00  \n",
      "2              100607       NO1    164912.02 2021-01-01 01:00:00+00:00  \n",
      "3              100607       NO1    160265.77 2021-01-01 02:00:00+00:00  \n",
      "4              100607       NO1    159828.69 2021-01-01 03:00:00+00:00  \n",
      "       consumptionGroup                   endTime           lastUpdatedTime  \\\n",
      "876595         tertiary 2024-12-31 19:00:00+00:00 2025-03-30 16:36:55+00:00   \n",
      "876596         tertiary 2024-12-31 20:00:00+00:00 2025-03-30 16:36:55+00:00   \n",
      "876597         tertiary 2024-12-31 21:00:00+00:00 2025-03-30 16:36:55+00:00   \n",
      "876598         tertiary 2024-12-31 22:00:00+00:00 2025-03-30 16:36:55+00:00   \n",
      "876599         tertiary 2024-12-31 23:00:00+00:00 2025-03-30 16:36:55+00:00   \n",
      "\n",
      "        meteringPointCount priceArea  quantityKwh                 startTime  \n",
      "876595               29584       NO5    373752.66 2024-12-31 18:00:00+00:00  \n",
      "876596               29584       NO5    360142.56 2024-12-31 19:00:00+00:00  \n",
      "876597               29584       NO5    349349.44 2024-12-31 20:00:00+00:00  \n",
      "876598               29584       NO5    332428.20 2024-12-31 21:00:00+00:00  \n",
      "876599               29584       NO5    321213.88 2024-12-31 22:00:00+00:00  \n"
     ]
    }
   ],
   "source": [
    "# Creating a pandas dataframe\n",
    "import pandas as pd\n",
    "consumption_df=pd.DataFrame(consumption_data)\n",
    "\n",
    "# Converting startTime, endTime and lastUpdatedTime to datetime and setting the timezone to UTC \n",
    "consumption_df['startTime'] = pd.to_datetime(consumption_df['startTime'], utc=True)\n",
    "consumption_df['endTime'] = pd.to_datetime(consumption_df['endTime'], utc=True)\n",
    "consumption_df['lastUpdatedTime'] = pd.to_datetime(consumption_df['lastUpdatedTime'], utc=True)\n",
    "#Displaying few rows to verify \n",
    "print(consumption_df.head())\n",
    "print(consumption_df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8734cf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cassandra setup \n",
    "import time\n",
    "from cassandra.cluster import Cluster\n",
    "cluster = Cluster(['localhost'], port=9042)\n",
    "session = cluster.connect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08cf0e66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cassandra.cluster.ResultSet at 0x1d6c348d310>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Setting up a cassandra keyspace  \n",
    "\n",
    "# Use the existing keyspace\n",
    "session.set_keyspace(\"ind_320_d2\")\n",
    "\n",
    "#Setting a table in the keyspace and ensuring it didnt exist before\n",
    "session.execute(\"DROP TABLE IF EXISTS ind_320_d2.consumption_table;\")\n",
    "\n",
    "#Making sure that column names are entact so using case sensitive format\n",
    "session.execute(\"CREATE TABLE IF NOT EXISTS consumption_table (\"\n",
    "                \"\\\"startTime\\\" timestamp, \"\n",
    "                \"\\\"endTime\\\" timestamp, \"\n",
    "                \"\\\"lastUpdatedTime\\\" timestamp, \"\n",
    "                \"\\\"priceArea\\\" text, \"\n",
    "                \"\\\"consumptionGroup\\\" text, \"\n",
    "                \"\\\"meteringPointCount\\\" int, \"\n",
    "                \"\\\"quantityKwh\\\" double, \"  \n",
    "                \"PRIMARY KEY ((\\\"priceArea\\\", \\\"consumptionGroup\\\"), \\\"startTime\\\")) \"\n",
    "                \"WITH CLUSTERING ORDER BY (\\\"startTime\\\" ASC);\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a97ffd46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session created successfully: 3.5.1\n"
     ]
    }
   ],
   "source": [
    "#Creating a Spark Cassandra Session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('Ind_320App').\\\n",
    "    config('spark.jars.packages', 'com.datastax.spark:spark-cassandra-connector_2.12:3.5.1').\\\n",
    "    config('spark.cassandra.connection.host', 'localhost').\\\n",
    "    config('spark.sql.extensions', 'com.datastax.spark.connector.CassandraSparkExtensions').\\\n",
    "    config('spark.sql.catalog.mycatalog', 'com.datastax.spark.connector.datasource.CassandraCatalog').\\\n",
    "    config('spark.cassandra.connection.port', '9042').getOrCreate()\n",
    "print(\"Spark session created successfully:\", spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57a60ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- consumptionGroup: string (nullable = true)\n",
      " |-- endTime: timestamp (nullable = true)\n",
      " |-- lastUpdatedTime: timestamp (nullable = true)\n",
      " |-- meteringPointCount: long (nullable = true)\n",
      " |-- priceArea: string (nullable = true)\n",
      " |-- quantityKwh: double (nullable = true)\n",
      " |-- startTime: timestamp (nullable = true)\n",
      "\n",
      "+----------------+-------------------+-------------------+------------------+---------+-----------+-------------------+\n",
      "|consumptionGroup|            endTime|    lastUpdatedTime|meteringPointCount|priceArea|quantityKwh|          startTime|\n",
      "+----------------+-------------------+-------------------+------------------+---------+-----------+-------------------+\n",
      "|           cabin|2021-01-01 01:00:00|2024-12-20 10:35:40|            100607|      NO1|  177071.56|2021-01-01 00:00:00|\n",
      "|           cabin|2021-01-01 02:00:00|2024-12-20 10:35:40|            100607|      NO1|  171335.12|2021-01-01 01:00:00|\n",
      "|           cabin|2021-01-01 03:00:00|2024-12-20 10:35:40|            100607|      NO1|  164912.02|2021-01-01 02:00:00|\n",
      "|           cabin|2021-01-01 04:00:00|2024-12-20 10:35:40|            100607|      NO1|  160265.77|2021-01-01 03:00:00|\n",
      "|           cabin|2021-01-01 05:00:00|2024-12-20 10:35:40|            100607|      NO1|  159828.69|2021-01-01 04:00:00|\n",
      "+----------------+-------------------+-------------------+------------------+---------+-----------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Converting to spark dataframe from pandas dataframe\n",
    "spark_consumption_df = spark.createDataFrame(consumption_df)\n",
    "\n",
    "#Displaying info about columns from Spark DataFrame to verify\n",
    "spark_consumption_df.printSchema()\n",
    "spark_consumption_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f203bdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inserting spark dataframe to cassandra, using .write to give data to cassandra\n",
    "spark_consumption_df.write.format(\"org.apache.spark.sql.cassandra\")\\\n",
    ".options(table=\"consumption_table\", keyspace=\"ind_320_d2\").mode(\"append\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9431afac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------------+-------------------+-------------------+-----------+------------------+\n",
      "|priceArea|consumptionGroup|          startTime|            endTime|quantityKwh|meteringPointCount|\n",
      "+---------+----------------+-------------------+-------------------+-----------+------------------+\n",
      "|      NO5|       secondary|2021-01-01 00:00:00|2021-01-01 01:00:00|  1094799.6|              5984|\n",
      "|      NO5|       secondary|2021-01-01 01:00:00|2021-01-01 02:00:00|  1099480.8|              5984|\n",
      "|      NO5|       secondary|2021-01-01 02:00:00|2021-01-01 03:00:00|  1054455.9|              5984|\n",
      "|      NO5|       secondary|2021-01-01 03:00:00|2021-01-01 04:00:00|  1049728.6|              5984|\n",
      "|      NO5|       secondary|2021-01-01 04:00:00|2021-01-01 05:00:00|  1099942.9|              5984|\n",
      "+---------+----------------+-------------------+-------------------+-----------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Data extraction from Cassandra into a Spark Dataframe, using .load() to load data from Cassandra as a Spark DataFrame.\n",
    "extracted_df_cons=spark.read.format(\"org.apache.spark.sql.cassandra\").options(table=\"consumption_table\", keyspace=\"ind_320_d2\").load().select(\"priceArea\", \"consumptionGroup\", \"startTime\",\"endTime\",\"quantityKwh\",\"meteringPointCount\")\n",
    "extracted_df_cons.show(5) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee258c14",
   "metadata": {},
   "source": [
    "### MongoDb Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ece55f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pinged your deployment. You successfully connected to MongoDB!\n"
     ]
    }
   ],
   "source": [
    "#Connecting to mongodb\n",
    "\n",
    "from pymongo.mongo_client import MongoClient\n",
    "from pymongo.server_api import ServerApi\n",
    "\n",
    "username,password = open(r\"D:\\NMBU\\semester_1\\IND-320\\mongodb_password.txt\").read().strip().split(',')\n",
    "\n",
    "uri = f\"mongodb+srv://{username}:{password}@app-cluster.ihj1zbx.mongodb.net/?retryWrites=true&w=majority&appName=app-cluster\"\n",
    "\n",
    "# Create a new client and connect to the server\n",
    "client = MongoClient(uri, server_api=ServerApi('1'))\n",
    "\n",
    "# Send a ping to confirm a successful connection\n",
    "try:\n",
    "    client.admin.command('ping')\n",
    "    print(\"Pinged your deployment. You successfully connected to MongoDB!\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c82bb288",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining a database and collection inside the cluster \n",
    "database=client['ind320_production_db']\n",
    "collection=database['ind320_production_table_d4']\n",
    "collection2=database['ind320_consumption_table']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ecc1e91a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark data inserted into MongoDB successfully!\n"
     ]
    }
   ],
   "source": [
    "#Converting Spark Dataframe to Pandas Dataframe and dictionaries (PRODUCTION DATA)\n",
    "pandas_df = extracted_df_prod.toPandas()\n",
    "data_dict = pandas_df.to_dict(\"records\")\n",
    "\n",
    "#Inserting into MongoDB\n",
    "collection.insert_many(data_dict)\n",
    "\n",
    "print(\"Spark data inserted into MongoDB successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eeb07703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark data inserted into MongoDB successfully!\n"
     ]
    }
   ],
   "source": [
    "#Converting Spark Dataframe to Pandas Dataframe and dictionaries(CONSUMPTION DATA)\n",
    "pandas_df_cons = extracted_df_cons.toPandas()\n",
    "data_dict_cons = pandas_df_cons.to_dict(\"records\")\n",
    "\n",
    "#Inserting into MongoDB\n",
    "collection2.insert_many(data_dict_cons)\n",
    "\n",
    "print(\"Spark data inserted into MongoDB successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ind320_try1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
